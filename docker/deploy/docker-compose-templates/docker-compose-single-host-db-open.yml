version: "3.8"
services:
# Service names should comply with the rules of building DNS names - they will be available
# under these domain names inside the cluster virtual network.
# (Service names should not contain underscores.)

  contrax-webdav:
    image: ${DOCKER_WEBDAV_IMAGE}
    volumes:
      - contraxsuite_data_media:/var/lib/dav
    networks:
      - contrax_net
    environment:   # No need for auth - in prod envs the server is not accessible from outside of our network
      AUTH_TYPE: Basic
      USERNAME: ${DOCKER_WEBDAV_AUTH_USER}
      PASSWORD: ${DOCKER_WEBDAV_AUTH_PASSWORD}
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  contrax-minio:
    hostname: contrax-minio
    image: ${DOCKER_MINIO_IMAGE}
    volumes:
      - contraxsuite_minio:/data
    networks:
      - contrax_net
    entrypoint: sh
    # Here we create a bucket dir for the mlflow models and drop minio config for the case we changed access/secret key.
    # Minio encrypts its config with access/secret keys and if they are changed it won't start until manual handling.
    command: -c "mkdir -p /data/${MLFLOW_AWS_BUCKET} && rm -rf /data/.minio.sys && /usr/bin/minio server /data"
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    ports:
      - 19000:9000
    environment:
      - "MINIO_ACCESS_KEY=${MLFLOW_AWS_ACCESS_KEY}"
      - "MINIO_SECRET_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "MINIO_BROWSER=off"
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  contrax-mlflow-tracking:
    image: ${DOCKER_MLFLOW_TRACKING_IMAGE}
    volumes:
      - contraxsuite_mlflow_tracking:/mlflow
    networks:
      - contrax_net
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    ports:
      - 15000:5000
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    environment:
      - "PORT=5000"
      - "FILE_DIR=/mlflow"
      - "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"
      - "AWS_BUCKET=${MLFLOW_AWS_BUCKET}"
      - "AWS_ACCESS_KEY_ID=${MLFLOW_AWS_ACCESS_KEY}"
      - "AWS_SECRET_ACCESS_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
  # There are 3 curator serivices deleting old log indexes here - 2 for filebeat and one for metricbeat.
  # This is done because there was an existing simple cron+curator docker image which supports
  # only specifying a single index prefix. It doesn't use a lot of memory and debugging/maintaining
  # more complicated config is worse than having two services.
  contrax-curator_filebeat:
    image: ${DOCKER_CURATOR_IMAGE}
    networks:
      - contrax_net
    environment:
      - "PERIOD=15min"
      - "KEEP_DAYS=20"
      - "INDEX_PREFIX=filebeat-logs-"
    command: "--host ${DOCKER_HOST_NAME_ELASTICSEARCH} --port ${DOCKER_ELASTICSEARCH_PORT}"
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  contrax-curator_filebeat-cx:
    image: ${DOCKER_CURATOR_IMAGE}
    networks:
      - contrax_net
    environment:
      - "PERIOD=15min"
      - "KEEP_DAYS=60"
      - "INDEX_PREFIX=filebeat-cx-"
    command: "--host ${DOCKER_HOST_NAME_ELASTICSEARCH} --port ${DOCKER_ELASTICSEARCH_PORT}"
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  contrax-curator_metricbeat:
    image: ${DOCKER_CURATOR_IMAGE}
    networks:
      - contrax_net
    environment:
      - "PERIOD=15min"
      - "KEEP_DAYS=3"
      - "INDEX_PREFIX=metricbeat-"
    command: "--host ${DOCKER_HOST_NAME_ELASTICSEARCH} --port ${DOCKER_ELASTICSEARCH_PORT}"
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  contrax-elasticsearch:
    image: ${DOCKER_ELASTICSEARCH_IMAGE}
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    configs:
      - source: elasticsearch_${ELASTICSEARCH_CONFIG_VERSION}
        target: /usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - contrax_net
    deploy:
      replicas: ${DOCKER_ELASTICSEARCH_REPLICAS}
      resources:
        limits:
          cpus: '${DOCKER_ELASTICSEARCH_CPUS}'
          memory: ${DOCKER_ELASTICSEARCH_MEMORY}
      placement:
        constraints: [node.role == manager]
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    environment:
      # this is to stop ES from printing some related error message every few seconds
      - "ES_JAVA_OPTS=-Des.transport.cname_in_publish_address=true"

  contrax-elastalert:
    # 2020-02-03.
    # Current "latest" version is 2.0.1 and it has an error fixed in 3.0.0-beta.1
    # and there is no release (non-beta) image yet.
    image: ${DOCKER_ELASTALERT_IMAGE}
    user: root
    volumes:
      - elastalert_rules:/rules
      - elastalert_rule_templates:/rule_templates
    configs:
      - source: elastalert_conf_${ELASTALERT_CONFIG_VERSION}
        target: /opt/elastalert/config.yaml
      - source: elastalert_conf_${ELASTALERT_CONFIG_VERSION}
        target: /opt/elastalert-server/config/elastalert.yaml
      - source: elastalertsrv_conf_${ELASTALERT_SERVER_CONFIG_VERSION}
        target: /opt/elastalert-server/config/config.json
      - source: elastalert_smtp_${ELASTALERT_SMTP_AUTH_VERSION}
        target: /elastalert-smtp-auth.yaml
    networks:
      - contrax_net
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '${DOCKER_ELASTALERT_CPUS}'
          memory: ${DOCKER_ELASTALERT_MEMORY}
      placement:
        constraints: [node.role == manager]
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    ports:
      - 3030:3030
      - 3333:3333

  contrax-kibana:
    image: ${DOCKER_KIBANA_IMAGE}
    networks:
      - contrax_net
    configs:
      - source: kibana_${KIBANA_CONFIG_VERSION}
        target: /usr/share/kibana/config/kibana.yml
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  contrax-filebeat:
    image: ${DOCKER_FILEBEAT_IMAGE}
    user: root
    volumes:
      - filebeat_data:/usr/share/filebeat/data
      - /:/hostfs:ro
      - /var/run/docker.sock:/var/run/docker.sock
    configs:
      - source: filebeat_${FILEBEAT_CONFIG_VERSION}
        target: /usr/share/filebeat/filebeat.yml
    networks:
      - contrax_net
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.5'
          memory: 1GB
      restart_policy:
        condition: on-failure

  contrax-metricbeat:
    image: ${DOCKER_METRICBEAT_IMAGE}
    volumes:
      - metricbeat_data:/usr/share/metricbeat/data
      - /var/run/docker.sock:/hostfs/var/run/docker.sock
      - /proc:/hostfs/proc:ro
      - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro
      - /:/hostfs:ro
    command: -e -system.hostfs=/hostfs
    user: root
    configs:
      - source: metricbeat_${METRICBEAT_CONFIG_VERSION}
        target: /usr/share/metricbeat/metricbeat.yml
    networks:
      - contrax_net
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      mode: global
      # replicas: 0 # by default we disable mertricbeat for single-host configs to free more resources
      resources:
        limits:
          cpus: '1'
          memory: 2GB

  contrax-rabbitmq:
    image: ${DOCKER_RABBITMQ_IMAGE}
    hostname: contrax-rabbitmq
    networks:
      - contrax_net
    environment:
      - "RABBITMQ_DEFAULT_USER=${DOCKER_RABBITMQ_USER}"
      - "RABBITMQ_DEFAULT_PASS=${DOCKER_RABBITMQ_PASSWORD}"
      - "RABBITMQ_DEFAULT_VHOST=${DOCKER_RABBITMQ_VHOST}"
    deploy:
      replicas: ${DOCKER_RABBITMQ_REPLICAS}
      placement:
        constraints: [node.role == manager]
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq

  contrax-pgbouncer-transaction:
    image: ${DOCKER_PGBOUNCER_IMAGE}
    networks:
      - contrax_net
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    configs:
      - source: pgbnc_celery_${PGBOUNCER_CELERY_CONFIG_VERSION}
        target: /etc/pgbouncer/pgbouncer.ini
      - source: pgbnc_userlist_${PGBOUNCER_USERLIST_VERSION}
        target: /etc/pgbouncer/userlist.txt
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    healthcheck:
      test: pg_isready -h ${DOCKER_HOST_NAME_PG} -U ${DOCKER_PG_USER} -d ${DOCKER_PG_DB_NAME} || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  contrax-pgbouncer-session:
    image: ${DOCKER_PGBOUNCER_IMAGE}
    networks:
      - contrax_net
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    configs:
      - source: pgbnc_websrv_${PGBOUNCER_WEBSRV_CONFIG_VERSION}
        target: /etc/pgbouncer/pgbouncer.ini
      - source: pgbnc_userlist_${PGBOUNCER_USERLIST_VERSION}
        target: /etc/pgbouncer/userlist.txt
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    healthcheck:
      test: pg_isready -h ${DOCKER_HOST_NAME_PG} -U ${DOCKER_PG_USER} -d ${DOCKER_PG_DB_NAME} || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  contrax-db:
    image: ${DOCKER_PG_IMAGE}
    environment:
      - "PGDATA=/var/lib/contraxsuite-postgres"
      - "POSTGRES_USER=${DOCKER_PG_USER}"
      - "POSTGRES_PASSWORD=${DOCKER_PG_PASSWORD}"
      - "POSTGRES_DB=${DOCKER_PG_DB_NAME}"
    command: bash -c "${POWA_INSTALL} service cron start && /usr/local/bin/docker-entrypoint.sh postgres -c config_file=/contraxsuite/postgresql.conf"
    tmpfs:
      - /tmp:size=4GB
    volumes:
      - postgres_data_11:/var/lib/contraxsuite-postgres
      - backup:/backup
      - type: tmpfs
        target: /dev/shm
    configs:
      - source: pg_${PG_CONFIG_VERSION}
        target: /contraxsuite/postgresql.conf
      - source: pg_backup_script_${PG_BACKUP_SCRIPT_CONFIG_VERSION}
        target: /contraxsuite/db-backup.sh
      - source: pg_backup_cron_${PG_BACKUP_CRON_CONFIG_VERSION}
        target: /etc/crontab
      - source: pg_init_sql_${PG_INIT_SQL_CONFIG_VERSION}
        target: /docker-entrypoint-initdb.d/postgres_init.sql
    networks:
      - contrax_net
    deploy:
      replicas: ${DOCKER_PG_REPLICAS}
      placement:
        constraints: [node.role == manager]
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    ports:
      - 0.0.0.0:54320:5432

  contrax-celery:
    image: ${CONTRAXSUITE_IMAGE}
    networks:
      - contrax_net
    command: ["/start.sh",  "celery-single"]
    environment:
      - "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"
      - "MLFLOW_AWS_ACCESS_KEY=${MLFLOW_AWS_ACCESS_KEY}"
      - "MLFLOW_AWS_SECRET_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "DOCKER_CELERY_CONCURRENCY=${CPU_HALF_CORES}"
      - "SHARED_USER_NAME=${SHARED_USER_NAME}"
      - "SHARED_USER_ID=${SHARED_USER_ID}"
      - "STARTUP_DEPS_READY_CMD=pg_isready -h ${DJANGO_CELERY_DB_HOST} -U ${DJANGO_CELERY_DB_USER} -d ${DOCKER_PG_DB_NAME}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
      - "AIOREDIS_DEBUG=${AIOREDIS_DEBUG}"
    configs:
      - source: ls_celery_${LOCAL_SET_CELERY_CONFIG_VERSION}
        target: /contraxsuite_services/local_settings.py
    volumes:
      - celery_worker_state:/data/celery_worker_state
    deploy:
      mode: global
      resources:
        limits:
          cpus: '${CPU_HALF_CORES}'
          memory: ${RAM_HALF_MB}M
    stop_signal: SIGQUIT
    stop_grace_period: 1m
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    healthcheck:
      test: pg_isready -h ${DJANGO_CELERY_DB_HOST} -U ${DJANGO_CELERY_DB_USER} -d ${DOCKER_PG_DB_NAME} || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  contrax-celery-doc-load:
    image: ${CONTRAXSUITE_IMAGE}
    networks:
      - contrax_net
    command: ["/start.sh",  "celery-load"]
    environment:
      - "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"
      - "MLFLOW_AWS_ACCESS_KEY=${MLFLOW_AWS_ACCESS_KEY}"
      - "MLFLOW_AWS_SECRET_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "SHARED_USER_NAME=${SHARED_USER_NAME}"
      - "SHARED_USER_ID=${SHARED_USER_ID}"
      - "DOCKER_CELERY_CONCURRENCY=1"
      - "STARTUP_DEPS_READY_CMD=pg_isready -h ${DJANGO_CELERY_DB_HOST} -U ${DJANGO_CELERY_DB_USER} -d ${DOCKER_PG_DB_NAME}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
      - "AIOREDIS_DEBUG=${AIOREDIS_DEBUG}"
    configs:
      - source: ls_celery_${LOCAL_SET_CELERY_CONFIG_VERSION}
        target: /contraxsuite_services/local_settings.py
    volumes:
      - celery_worker_state:/data/celery_worker_state
    deploy:
      mode: global # Exactly one instance per node. Primitive AWS autoscaling solution.
    stop_signal: SIGQUIT
    stop_grace_period: 1m
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    healthcheck:
      test: pg_isready -h ${DJANGO_CELERY_DB_HOST} -U ${DJANGO_CELERY_DB_USER} -d ${DOCKER_PG_DB_NAME} || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  contrax-celery-beat:
    image: ${CONTRAXSUITE_IMAGE}
    networks:
      - contrax_net
    command: ["/start.sh",  "celery-beat"]
    environment:
      - "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"
      - "MLFLOW_AWS_ACCESS_KEY=${MLFLOW_AWS_ACCESS_KEY}"
      - "MLFLOW_AWS_SECRET_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "SHARED_USER_NAME=${SHARED_USER_NAME}"
      - "SHARED_USER_ID=${SHARED_USER_ID}"
      - "STARTUP_DEPS_READY_CMD=pg_isready -h ${DJANGO_WEBSRV_DB_HOST} -U ${DJANGO_WEBSRV_DB_USER} -d ${DOCKER_PG_DB_NAME}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
      - "AIOREDIS_DEBUG=${AIOREDIS_DEBUG}"
    configs:
      - source: ls_celery_${LOCAL_SET_CELERY_CONFIG_VERSION}
        target: /contraxsuite_services/local_settings.py
    volumes:
      - celery_worker_state:/data/celery_worker_state
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    stop_signal: SIGQUIT
    stop_grace_period: 1m
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    healthcheck:
      test: pg_isready -h ${DJANGO_CELERY_DB_HOST} -U ${DJANGO_CELERY_DB_USER} -d ${DOCKER_PG_DB_NAME} || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  contrax-daphne:
    image: ${CONTRAXSUITE_IMAGE}
    networks:
      - contrax_net
    command: ["/start.sh",  "daphne"]
    environment:
      - "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"
      - "MLFLOW_AWS_ACCESS_KEY=${MLFLOW_AWS_ACCESS_KEY}"
      - "MLFLOW_AWS_SECRET_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "SHARED_USER_NAME=${SHARED_USER_NAME}"
      - "SHARED_USER_ID=${SHARED_USER_ID}"
      - "DOCKER_DJANGO_HOST_NAME=${DOCKER_DJANGO_HOST_NAME}"
      - "DOCKER_DJANGO_ADMIN_PASSWORD=${DOCKER_DJANGO_ADMIN_PASSWORD}"
      - "DOCKER_DJANGO_ADMIN_EMAIL=${DOCKER_DJANGO_ADMIN_EMAIL}"
      - "DOCKER_WEBDAV_SERVER_NAME=${DOCKER_WEBDAV_SERVER_NAME}"
      - "DOCKER_WEBDAV_AUTH_USER=${DOCKER_WEBDAV_AUTH_USER}"
      - "DOCKER_WEBDAV_AUTH_PASSWORD=${DOCKER_WEBDAV_AUTH_PASSWORD}"
      - "STARTUP_DEPS_READY_CMD=pg_isready -h ${DJANGO_WEBSRV_DB_HOST} -U ${DJANGO_WEBSRV_DB_USER} -d ${DOCKER_PG_DB_NAME}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
      - "AIOREDIS_DEBUG=${AIOREDIS_DEBUG}"
    volumes:
      - contraxsuite_third_party_dependencies:/third_party_dependencies
      - contraxsuite_frontend:/contraxsuite_frontend_nginx_volume
    configs:
      - source: ls_websrv_${LOCAL_SET_WEBSRV_CONFIG_VERSION}
        target: /contraxsuite_services/local_settings.py
      - source: uwsgi_ini_${UWSGI_INI_CONFIG_VERSION}
        target: /contraxsuite_services/uwsgi.ini
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    healthcheck:
      test: pg_isready -h ${DJANGO_WEBSRV_DB_HOST} -U ${DJANGO_WEBSRV_DB_USER} -d ${DOCKER_PG_DB_NAME} || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  contrax-nginx:
    image: ${DOCKER_NGINX_IMAGE}
    networks:
      - contrax_net
    volumes:
      - contraxsuite_nginx_conf:/etc/nginx
      - contraxsuite_nginx_certs:/etc/nginx/certificates
      - contraxsuite_frontend:/contraxsuite_frontend
      - contraxsuite_nginx_server_include:/etc/nginx/server_include
    configs:
      - source: nginx_cust_${NGINX_CUSTOMER_CONF_VERSION}
        target: /etc/nginx/nginx-customer.conf
    ports:
      # ports are configured this way to make Nginx running inside Docker Swarm receiving the real remote client address
      # https://stackoverflow.com/questions/49415595/docker-swarm-get-real-ip-client-host-in-nginx
      - mode: host
        protocol: tcp
        published: 80
        target: 8080
      - mode: host
        protocol: tcp
        published: 443
        target: 4443
      - mode: host
        protocol: tcp
        published: ${POWA_NGINX_PORT}
        target: 4444
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  contrax-jupyter:
    image: ${CONTRAXSUITE_IMAGE}
    networks:
      - contrax_net
    command: ["/start.sh",  "jupyter"]
    environment:
      - "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"
      - "MLFLOW_AWS_ACCESS_KEY=${MLFLOW_AWS_ACCESS_KEY}"
      - "MLFLOW_AWS_SECRET_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "SHARED_USER_NAME=${SHARED_USER_NAME}"
      - "SHARED_USER_ID=${SHARED_USER_ID}"
      - "DOCKER_DJANGO_ADMIN_PASSWORD=${DOCKER_DJANGO_ADMIN_PASSWORD}"
      - "STARTUP_DEPS_READY_CMD=pg_isready -h ${DJANGO_CELERY_DB_HOST} -U ${DJANGO_CELERY_DB_USER} -d ${DOCKER_PG_DB_NAME}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
      - "AIOREDIS_DEBUG=${AIOREDIS_DEBUG}"
    volumes:
      - contraxsuite_notebooks:/contraxsuite_services/notebooks/persistent_volume
      - contraxsuite_jupyter_add_req:/contraxsuite_services/jupyter_add_req
    configs:
      - source: ls_websrv_${LOCAL_SET_WEBSRV_CONFIG_VERSION}
        target: /contraxsuite_services/local_settings.py
      - source: jupyter_${JUPYTER_CONFIG_VERSION}
        target: /contraxsuite_services/jupyter_notebook_config.py
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '${DOCKER_JUPYTER_CPUS}'
          memory: ${DOCKER_JUPYTER_MEMORY}
      placement:
        constraints: [node.role == manager]

  contrax-flower:
    image: ${CONTRAXSUITE_IMAGE}
    networks:
      - contrax_net
    command: ["/start.sh",  "flower"]
    environment:
      - "MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}"
      - "MLFLOW_AWS_ACCESS_KEY=${MLFLOW_AWS_ACCESS_KEY}"
      - "MLFLOW_AWS_SECRET_KEY=${MLFLOW_AWS_SECRET_KEY}"
      - "SHARED_USER_NAME=${SHARED_USER_NAME}"
      - "SHARED_USER_ID=${SHARED_USER_ID}"
      - "DOCKER_FLOWER_BASE_PATH=${DOCKER_FLOWER_BASE_PATH}"
      - "STARTUP_DEPS_READY_CMD=pg_isready -h ${DJANGO_CELERY_DB_HOST} -U ${DJANGO_CELERY_DB_USER} -d ${DOCKER_PG_DB_NAME}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
      - "AIOREDIS_DEBUG=${AIOREDIS_DEBUG}"
    configs:
      - source: ls_websrv_${LOCAL_SET_WEBSRV_CONFIG_VERSION}
        target: /contraxsuite_services/local_settings.py
    deploy:
      replicas: 0 # by default we disable flower fo single-host deployments to free resources
      resources:
        limits:
          cpus: '${DOCKER_FLOWER_CPUS}'
          memory: ${DOCKER_FLOWER_MEMORY}
      placement:
        constraints: [node.role == manager]
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    sysctls:
      # Possible solution for DB connections disappearing.
      # https://forums.docker.com/t/setting-tcp-keepalive-time-param-for-containers/41496/3
      # Swarm Load Balancer purges idle connections after 900 seconds. Need keep-alive < 900 s.
      net.ipv4.tcp_keepalive_time: 600
      net.ipv4.tcp_keepalive_intvl: 60
      net.ipv4.tcp_keepalive_probes: 3

  contrax-redis:
    image: ${DOCKER_REDIS_IMAGE}
    volumes:
      - redis_data:/data
    networks:
      - contrax_net
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]

  contrax-powa-web:
    image: powateam/powa-web
    networks:
      - contrax_net
    configs:
      - source: powa_web_${POWA_WEB_CONFIG_VERSION}
        target: /etc/powa-web.conf
    container_name: powa-web
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    deploy:
      replicas: ${POWA_WEB_REPLICAS}

  contrax-mock-imanage:
    image: ${CONTRAXSUITE_IMAGE}
    networks:
      - contrax_net
    command: ["/start.sh",  "mock-imanage"]
    environment:
      - "SHARED_USER_NAME=${SHARED_USER_NAME}"
      - "SHARED_USER_ID=${SHARED_USER_ID}"
      - "http_proxy=${PROXY_SERVER_HTTP}"
      - "HTTP_PROXY=${PROXY_SERVER_HTTP}"
      - "https_proxy=${PROXY_SERVER_HTTPS}"
      - "HTTPS_PROXY=${PROXY_SERVER_HTTPS}"
      - "no_proxy=${PROXY_NO_PROXY}"
      - "NO_PROXY=${PROXY_NO_PROXY}"
    deploy:
      replicas: ${MOCK_IMANAGE_REPLICAS}
      placement:
        constraints: [node.role == manager]
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m
    volumes:
      - mock_imanage_data:/contraxsuite_services/apps/imanage_integration/debug/data/

volumes:
  contraxsuite_nginx_conf:
  contraxsuite_nginx_certs:
  contraxsuite_frontend:
  contraxsuite_third_party_dependencies:
  contraxsuite_data_media:
  contraxsuite_notebooks:
  postgres_data_11:
  rabbitmq_data:
  elasticsearch_data:
  redis_data:
  filebeat_data:
  metricbeat_data:
  backup:
  celery_worker_state:
  contraxsuite_nginx_server_include:
  contraxsuite_jupyter_add_req:
  contraxsuite_minio:
  contraxsuite_mlflow_tracking:
  elastalert_rules:
  elastalert_rule_templates:
  mock_imanage_data:

networks:
  contrax_net:

configs:
  filebeat_${FILEBEAT_CONFIG_VERSION}:
    file: ./filebeat.yml
  metricbeat_${METRICBEAT_CONFIG_VERSION}:
    file: ./metricbeat.yml
  elasticsearch_${ELASTICSEARCH_CONFIG_VERSION}:
    file: ./elasticsearch.yml
  elastalert_conf_${ELASTALERT_CONFIG_VERSION}:
    file: ./elastalert-config.yaml
  elastalertsrv_conf_${ELASTALERT_SERVER_CONFIG_VERSION}:
    file: ./elastalert-server-config.json
  elastalert_smtp_${ELASTALERT_SMTP_AUTH_VERSION}:
    file: ./elastalert-smtp-auth.yaml
  kibana_${KIBANA_CONFIG_VERSION}:
    file: ./kibana.yml
  pg_${PG_CONFIG_VERSION}:
    file: ./postgresql.conf
  pg_backup_script_${PG_BACKUP_SCRIPT_CONFIG_VERSION}:
    file: ./db-backup.sh
  pg_backup_cron_${PG_BACKUP_CRON_CONFIG_VERSION}:
    file: ./backup-cron.conf
  pg_init_sql_${PG_INIT_SQL_CONFIG_VERSION}:
    file: ./postgres_init.sql
  nginx_cust_${NGINX_CUSTOMER_CONF_VERSION}:
    file: ./nginx-customer.conf
  powa_web_${POWA_WEB_CONFIG_VERSION}:
    file: ./powa-web.conf
  ls_websrv_${LOCAL_SET_WEBSRV_CONFIG_VERSION}:
    file: ./local_settings_websrv.py
  ls_celery_${LOCAL_SET_CELERY_CONFIG_VERSION}:
    file: ./local_settings_celery.py
  pgbnc_websrv_${PGBOUNCER_WEBSRV_CONFIG_VERSION}:
    file: ./pgbouncer.websrv.ini
  pgbnc_celery_${PGBOUNCER_CELERY_CONFIG_VERSION}:
    file: ./pgbouncer.celery.ini
  uwsgi_ini_${UWSGI_INI_CONFIG_VERSION}:
    file: ./uwsgi.ini
  jupyter_${JUPYTER_CONFIG_VERSION}:
    file: ./jupyter_notebook_config.py
  pgbnc_userlist_${PGBOUNCER_USERLIST_VERSION}:
    file: ./pgbouncer.userlist.txt
